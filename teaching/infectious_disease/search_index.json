[["index.html", "Epidemiological Modelling at Cermel: Practical Notes Chapter 1 Practical notes for days 4 and 5 1.1 Copying code 1.2 What to work on?", " Epidemiological Modelling at Cermel: Practical Notes Bram Kuijper, Mario Recker 2024-03-01 Chapter 1 Practical notes for days 4 and 5 Here the practical exercises with hints to use during the tutorials and the questions for you to solve during the practical. 1.1 Copying code If you hover over the top right corner of a code block in these lecture notes, a copy button emerges that allows you to copy the code (and then paste it into Rstudio) without having to first select all of the code. 1.2 What to work on? Look at the sections that are listed as Task. Please try to solve those tasks and put resulting code and figures in a word doc or use an rmarkdown document to compile your answers. "],["practical-uncertainties.html", "Chapter 2 Practical: uncertainties 2.1 Estimating \\(R_{0}\\) in a risk-heterogenous population 2.2 Maximum likelihood and age-dependent seropositivity", " Chapter 2 Practical: uncertainties 2.1 Estimating \\(R_{0}\\) in a risk-heterogenous population In the first of this tutorial, we will run an SIS model of a population heterogenous for risk. We then estimate \\(R_{0}\\) for different WAIFW matrices (Who Acquires Infection From Whom) 2.1.1 SIS model code Inspect the model code below (particularly the set of differential equations) and compare the equations with the flow diagram of the SIS model in the lecture slides. library(&quot;tidyverse&quot;) library(&quot;deSolve&quot;) # set of differential equations # of a model in which classes vary # in their transmission risk SIS_structure &lt;- function(t, y, parameters) { # get state variables from # the y function argument SL = y[1] SH = y[2] IL = y[3] IH = y[4] # with() creates a &#39;sub environment&#39; # in which all the named values of the # parameters list are now variables by themselves # hence, we can then evaluate all the gradients # within this environment result &lt;- with(data=as.list(parameters), expr = { # evaluate each of the equations dSL &lt;- -mu * SL - betaLL * SL * IL - betaHL * SL * IH + gamma * IL dSH &lt;- -mu * SH - betaLH * SH * IL - betaHH * SH * IH + gamma * IH dIL &lt;- betaHL * SL * IH + betaLL * SL * IL - (gamma + mu) * IL dIH &lt;- betaLH * SH * IL + betaHH * SH * IH - (gamma + mu) * IH # then return the gradient values c(dSL,dSH,dIL,dIH) }) # return list of gradients return(list(result)) } # end SIR_structure # set out the time points times &lt;- seq(from = 0, to = 25, length.out = 1000) # make a named vector containing all # the parameters used in the model params &lt;- c( betaHH=5, betaHL=0.1, betaLH=0.1, betaLL=2, gamma=1, mu=0, N=1 ) # a set of starting values in which we # assume that equally frequent start &lt;- c(SL=0.495,SH=0.495,IL=0.005,IH=0.005) # run the model and obtain a data.frame # with densities over time output &lt;- as.data.frame(ode(y = start ,times = times ,func=SIS_structure ,parms = params)) 2.1.2 Task: estimate R0 without taking into account risk After having inspected the model code, try to run it. There should now be an output variable, which contains a data.frame. This data.frame contains the the time evolution of the densities of susceptibles and infecteds. Try to get the final epidemic size proportion of susceptibles from the last row of the output data.frame, by summing the low and high risk susceptibles. Use this number as your value of \\(p_{S}\\) and then calculate \\(R_{0}\\) from that (see the lecture slides for the corresponding formula). 2.1.3 Task: estimate R0 while taking into account risk Now we will try to obtain a more precise measure of \\(R_{0}\\), by using the WAIFW matrix and the equilibrium values of \\(S_{H}\\) and \\(S_{L}\\) which you obtained in the previous subsection. Use R’s matrix() command to fill out the following \\(2 \\times 2\\) matrix, using the densities \\(S_{H}\\), \\(S_{L}\\) and the parameter values of \\(\\mathbf{\\beta}\\) that you used to run the SIS model \\[\\begin{aligned} \\mathbf{R} &amp;= \\left [ \\begin{matrix} \\beta_{HH} S_{H}, \\beta_{HL} S_{H} \\\\ \\beta_{LH} S_{L}, \\beta_{LL} S_{L} \\end{matrix} \\right ] \\end{aligned}\\] Then use the eigen() command on this matrix to calculate the dominant (i.e., the largest) eigenvalue, which is \\(R_{0}\\) as it calculates the overall number of secondary cases, while taking into account the different contributions of high and low risk individuals. Associated to the dominant eigenvalue is the dominant eigenvector, absolute values of which inform one about the relative long-term contribution in spreading the epidemic by high versus low-risk individuals. 2.2 Maximum likelihood and age-dependent seropositivity For a disease with an unknown \\(R_{0}\\), we provide three datasets with data on whether individuals are seropositive or seronegative (i.e.,have antibodies or not) and that individual’s age. These datasets are similar to the three datasets shown during tutorial 7 on uncertainty. We will use this dataset to produce a maximum likelihood approach to estimate the value of \\(R_{0}\\) for each of these datasets. 2.2.1 The datasets You can download the datasets below by clicking on the csv button at the top of the table, save them to disk and then read them into R with the command read.table(). Make sure to choose a sensible file name. 2.2.2 Dataset 1: quite some susceptibles in early life 2.2.3 Dataset 2: only a few susceptibles in early life 2.2.4 Dataset 3: loss of immunity in later life 2.2.5 Probabilities and the likelihood function In order to obtain estimates of \\(R_{0}\\) we need to evaluate the probability of each data point given the currently picked value of \\(R_{0}\\). Let \\(P(a_{i}) = \\exp \\left [-a_{i}\\mu \\left (R_{0} - 1 \\right ) \\right ]\\) be the probability that the \\(i\\)th individual sampled of age \\(a_{i}\\) is still susceptible (i.e., has no antibodies). Similarly, \\(Q(a_{i}) = 1 - \\exp \\left [-a_{i}\\mu \\left (R_{0} - 1 \\right ) \\right ]\\) reflects the probability that someone is not susceptible. We will now use these probabilities to calculate the log-likelihood, using the following function: # function that calculates a likelihood # for: # - an R0 that you provide # - a data.frame data # with columns &#39;age&#39; and &#39;sero&#39; # reflecting ages of individuals and whether # they are seropositive # - mu, the mortality rate ll &lt;- function(R0, data, mu) { # initialize the log likelihood # by setting it to zero, we will # add to this value below loglik &lt;- 0 # go through all the individual values # by going through all the rows in the dataset for (row_idx in 1:nrow(data)) { # get the age of the currently considered # individual from the data.frame age &lt;- data[row_idx,&quot;age&quot;] # check wether the individual # was seropositive if (data[row_idx,&quot;sero&quot;] &gt; 0) { # if yes, the log likelihood # is log(1 - exp(-age * mu * (R0 - 1))) loglik &lt;- loglik + log(1 - exp(-age * mu * (R0 - 1))) } else # individual sero negative otherwise { # log(exp(-age * mu *(R0 - 1))) # = -age * mu * (R0 - 1) loglik &lt;- loglik - age * mu * (R0 - 1) } } # end for loop # returns a single number, which is the log # likelihood return(loglik) } # end ll() You see in the code above that we sum up the logs of these probabilities over all the datapoints, which gives us the log likelihood for a particular \\(R_{0}\\). 2.2.6 Task: calculating the log-likelihood for \\(R_{0} = 3\\) Now use the ll() function above by giving it \\(R_{0} = 3\\), \\(\\mu = 1/80\\) and the first dataset (the one that had quite a bit of early-life susceptibility) that you have downloaded above. The value that you should get out for \\(R_{0}\\) should be: ## [1] -151.4656 This log-likelihood value in itself does not tell you too much about which \\(R_{0}\\) is most likely to explain the data. We need to compare it to the log-likelihood values for other values of \\(R_{0}\\)! Question do you know why the log-likelihood is negative? 2.2.7 Task: calculate the log-likelihood for a range of \\(R_{0}\\) values Now use the ll() function to calculate the likelihood across a range of different \\(R_{0}\\) values. For example we can make a list of \\(R_{0}\\) values and feed it to the ll() function. Hint don’t forget to change the name your_data_set of your dataset below R0_range &lt;- seq(1,15,length.out=1000) llikelihood_vals &lt;- ll(R0 = R0_range, mu = 1/80, data=your_data_set) What do you conclude about the most likely \\(R_{0}\\) value that explains the data? 2.2.8 Task: calculate log likelihood for the other datasets Can you now repeat the above for all three datasets? What values of \\(R_{0}\\) that maximize the likelihood for each dataset do you find? Do these values make sense, you think? 2.2.9 Task: thinking further about the loss of immunity dataset Clearly, the \\(R_{0}\\) for the third data set is rather different. Why do you think this is? Can we perhaps avoid the loss of immunity complications, by only focusing on the younger ages (e.g., up to 60)? Select the data so that only individuals younger than 60 years old are included in the dataset, for example by performing: younger_subset &lt;- my_dataset %&gt;% filter(age &lt; 60) and then repeat the likelihood calculations through ll(). What is the value of \\(R_{0}\\) for this younger_subset dataset? What do you conclude about the impact of immunity loss for our calculations? "],["practical-probabilistic-models-in-disease-evolution.html", "Chapter 3 Practical: probabilistic models in disease evolution 3.1 Gamma chain models 3.2 Working with Gillespie models", " Chapter 3 Practical: probabilistic models in disease evolution Here we will focus on two applications: 1. A so-called ‘gamma-chain’ model to explore the impact of more realistic recovery distributions. 2. The famous Gillespie Model of Stochastic disease modelling, to track probabilistic changes in densities. 3.1 Gamma chain models During the lectures, we discussed that more realistic distributions of the recovery time \\(\\sigma\\) may change the spread of the disease onset as well as peak numbers of infected. Here, we will demonstrate this ourselves by running our own gamma chain model. A key parameter here is \\(n\\), the number of stages through which an individual has to pass before clearing the disease. This could, for example, reflect the time it takes before the immune system kicks in or before treatment finally starts to work and clear the disease. 3.1.1 Gamma chain model code library(&quot;deSolve&quot;) library(&quot;tidyverse&quot;) ## ── Attaching core tidyverse packages ──────────────────────────────────── tidyverse 2.0.0 ── ## ✔ dplyr 1.1.4 ✔ readr 2.1.5 ## ✔ forcats 1.0.0 ✔ stringr 1.5.1 ## ✔ ggplot2 3.4.4 ✔ tibble 3.2.1 ## ✔ lubridate 1.9.3 ✔ tidyr 1.3.1 ## ✔ purrr 1.0.2 ## ── Conflicts ────────────────────────────────────────────────────── tidyverse_conflicts() ── ## ✖ dplyr::filter() masks stats::filter() ## ✖ dplyr::lag() masks stats::lag() ## ℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors # incorporate a gamma distribution # of infection time # into an SIR model sir_chain_mod &lt;- function(t, init_densities, parameters) { # vector with all the densities # received via the command line x &lt;- init_densities # derive the total population size from # the sum of the densities N &lt;- sum(init_densities) # number of time steps an individual needs # to undergo before the infection is cleared n &lt;- parameters[&quot;n&quot;] # we need at least one infected stage stopifnot(n &gt;= 1) # some bounds checking # we expect one density for S and R each # and n densities for I, making n+2 stopifnot(length(x) == n + 2) # initial density of S, this is only a single value S &lt;- x[1] # initial density of I, because we have now # different compartments this is a whole # range of values I &lt;- x[2:(n + 1)] # then finally get the fraction of resistant # individuals from the population density vector R &lt;- x[n + 2] with(as.list(parameters), { total_I &lt;- sum(I) # dS/dt is just the same as # it ever was dS &lt;- mu * N - beta * S * total_I / N - mu * S # allocate empty space filled # with zeros for the n dI/dts dI &lt;- numeric(length = n) # dI1/dt is a bit special as it receives # susceptibles whereas dI2/dt etc do not # hence write dI1/dt outside # the for loop dI[1] &lt;- beta * S * total_I / N - (mu + n * sigma) * I[1] # evaluate all the the dI_i equations if (n &gt; 1) { for (i in 2:n) { dI[i] &lt;- n * sigma * I[i - 1] - (n * sigma + mu) * I[i] } } # and finally dR/dt dR &lt;- n * sigma * I[n] - mu * R result &lt;- c(dS, dI, dR) return(list(result)) }) } # end sir_chain_mod() # first define all parameters but n, # as we will vary n below parameters &lt;- c(sigma = 1/20, beta = 1, mu = 1/75) # now make a copy to use for the model # with one infectious class, in which we specify # n = 1 parameters.n1 &lt;- c(n = 1, parameters) # once we have the parameters, # produce a vector of initial densities # pay attention to how we initialize I # there are now n different infected types # with I1 = 1, all other I = 0 as initial density initial_densities = c(S = 9999, I = c(1, numeric(length = parameters.n1[&quot;n&quot;]-1)), R = 0 ) # set the sequence of time points to solve # the gamma chain ode over times &lt;- seq(0,250, by =0.1) # run the model for n = 1 result.n1 &lt;- as.data.frame(ode(y = initial_densities, times = times, func = sir_chain_mod, parms =parameters.n1)) # now run the model for a higher # number of classes that infecteds # have to pass through, ie., n &gt; 1 parameters.nhigher &lt;- c(n = 3, parameters) # again set initial densities initial_densities = c(S = 9999, I = c(1, numeric(length = parameters.nhigher[&quot;n&quot;]-1)), R = 0 ) # capture the results of the model result.nhigher &lt;- as.data.frame(ode(y = initial_densities, times = times, func = sir_chain_mod, parms =parameters.nhigher)) # now we need to get the total number of infecteds # across all the classes # we also label the rows of the dataset result.n1 &lt;- result.n1 %&gt;% mutate( totalI = result.n1 %&gt;% select(starts_with(&quot;I&quot;)) %&gt;% rowSums(), type = &quot;n = 1&quot; ) # now we need to get the total number of infecteds # across all the classes result.nhigher &lt;- result.nhigher %&gt;% mutate( totalI = result.nhigher %&gt;% select(starts_with(&quot;I&quot;)) %&gt;% rowSums(), type = &quot;n &gt; 1&quot; ) # finally we need to combine both data.frames # so that we can plot the results overall.data &lt;- bind_rows( result.n1 %&gt;% select(c(time, totalI, type)), result.nhigher %&gt;% select(c(time, totalI, type)) ) ggplot(data = overall.data, mapping= aes(x = time, y = totalI)) + geom_line(mapping = aes(colour=type)) + ylab(&quot;Total density of I&quot;) + xlab(&quot;Time&quot;) + theme_classic() 3.1.2 Task: run the gamma chain model Copy the code above to a clean script window in Rstudio and run it. Can you look the parameters above and find the value of \\(R_{0}\\) = \\(\\beta\\) / \\(\\sigma\\) ? Do you see any differences between \\(n = 1\\) (the classical SIR) and \\(n &gt; 1\\) (the gamma chain model)? 3.1.3 Task: lower the \\(R_{0}\\) value in the gamma chain model Now run the same model again, but for lower values of \\(R_{0}\\) than previously considered. For example, by setting \\(\\sigma = 1/4\\) while maintaining \\(\\beta = 1\\) (what is \\(R_{0}\\) now?). You should now see that both lines fluctuate before settling on an equilibrium. However, what is different about the fluctuations for \\(n &gt; 1\\) versus \\(n = 1\\)? Why do you think this is? 3.1.4 Task: increase \\(n\\) Now increase \\(n\\) to, for example, \\(n = 10\\), so that individuals need to pass through more stages before clearing the disease. Compare your results to the previous task. What has changed? 3.1.5 Extra: trouble arises when one increases \\(n\\) even further If you would change \\(n\\) to even larger values something ‘interesting’ happens, namely that ode() now often fails to solve the model or gives spurious results. This happens because the densities of infecteds are now distributed over so many classes that the per-class densities of \\(I_{i}\\) simply become too low to solve for ode(). Slightly more advanced methods are needed, for example by transforming the densities to a log scale (e.g., see the code in Bjørnstad 2023, p. 27). Those of you really interested in this, try to change the model code above to 3.2 Working with Gillespie models Gillespie models are a fantastic way to develop stochastic formulations of ordinary differential equations, including the SIR model. Their area of application is enormous, from quan 3.2.1 Gillespie model code Here the code that will be covered during the practical. If you compare this to a deterministic model what do you see? # SIR model Gillespie library(&quot;tidyverse&quot;) # develops the various rates of the SIR models # and the resulting changes in density of S, I or R stoch_eqns &lt;- function(input, params) { # rows of a data frame are in list() # format which may give trouble. Hence # I use unlist() input &lt;- unlist(input) # the current value of time ts &lt;- input[1] # the initial densities dens &lt;- input[2:4] # pre-allocate a # 6 x 3 matrix reflecting the change in densities of # S, I or R during one event density_change_matrix &lt;- matrix( data=0 # fill it with zeros ,nrow=6 # 6 events hence six rows ,ncol=length(dens)) # allocate a matrix for all the density changes density_change_matrix &lt;- matrix(data=0, nrow = 6, ncol = 3, byrow = T) # total density at the start Ntotal &lt;- sum(dens) # extinction if (Ntotal &lt;= 0) { # return the current time steps and all # densities set to 0 return(c(ts,numeric(length=length(dens)))) } # then fill the rates vector rates &lt;- numeric(length = 6) # normally we use with() to make # the parameters named vector into # variables, but with() is slooooow lambda = parameters[&quot;lambda&quot;] beta = parameters[&quot;beta&quot;] mu = parameters[&quot;mu&quot;] sigma = parameters[&quot;sigma&quot;] # event 1: birth of susceptible rates[1] &lt;- lambda * Ntotal density_change_matrix[1,] &lt;- c(1,0,0) # event 2: infection of susceptible rates[2] &lt;- beta * dens[1] * dens[2] / Ntotal # beta * X * Y N, infection density_change_matrix[2,] &lt;- c(-1,1,0) # event 3: death of susceptible rates[3] &lt;- mu * dens[1] # mu X density_change_matrix[3,] &lt;- c(-1,0,0) # event 4: loss of infection rates[4] &lt;- sigma * dens[2] # sigma Y density_change_matrix[4,] &lt;- c(0,-1,1) # event 5: death of infected rates[5] &lt;- mu * dens[2] # mu Y density_change_matrix[5,] &lt;- c(0,-1,0) # event 6: death of resistant rates[6] &lt;- mu * dens[3] # mu Z, density_change_matrix[6,] &lt;- c(0,0,-1) # calculate the total rate sum_rates &lt;- sum(rates) # random uniformly distributed number # between 0 and 1 rand1 &lt;- runif(n=1) # calculate time that nothing happens ts &lt;- ts -log(rand1) / (sum_rates) # bounds checking, rates cannot be negative stopifnot(rates &gt;= 0) # choose which of 6 events # each event has its own &#39;weight&#39; that determines # how likely it is to be chosen which_event &lt;- sample(x = 1:6, # a number between 1 and 6 size = 1, prob = rates # weights given by rates ) # materialise the actual change in the densities dens = dens + density_change_matrix[which_event,] # in case newly calculated # densities are negative # set them to zero dens[which(dens &lt; 0)] &lt;- 0 # return both the densities and the time that nothing happened return(c(ts,dens)) } # end stoch eqns SIR_Gillespie &lt;- function(params, initial_values, max_time=100) { if (max_time &gt; 1e03) { print(&quot;Choose max_time to be a smaller value unless you want to wait (very long)&quot;) } # we may have to go through a large number # of time steps, but we stop as soon as the # actual time &gt; max_time max_steps &lt;- 1e05 # pre-allocate data for the numerical output # very difficult to predict size needed as # by chance, some simulations may involve # more events than others variables &lt;- as.data.frame(matrix(data =0, # fill with zeros nrow=max_steps, # max_steps rows ncol=5)) # give names to the columns # notice that we have time as the total # number of events that happened (T) # and actual time (total number of events # * time steps in between events) captured # in the variable time colnames(variables) &lt;- c(&quot;T&quot;,&quot;time&quot;,&quot;S&quot;,&quot;I&quot;,&quot;R&quot;) # set initial time and initial values variables[1,] &lt;- c(0, 0, initial_values) # keep track of the real time that accumulates # between events real_time &lt;- 0 # keep track of counter of the number of events step_idx &lt;- 2 # now iterate the system for max_time steps while (real_time &lt; max_time) { # evaluate the stochastic equations result &lt;- stoch_eqns( input=variables[step_idx - 1,2:5], params = params) # update the current time real_time &lt;- result[1] # update all the variables # from the stochastic equations variables[step_idx,] &lt;- c(step_idx, result) # if all the densities are 0 # then population extinct, quit if (sum(result[2:length(result)]) &lt;= 0 || real_time &gt; max_time) { # drop the remainder of the variables # table as it will be unfilled anyway variables &lt;- variables[1:step_idx,] # the final event # has happened after max_time # at max_time itself the state of # the population is the same # as previous variables[step_idx,] &lt;- variables[step_idx -1,] variables[step_idx,&quot;time&quot;] &lt;- max_time break } # increase data.frame size if needed if (step_idx == nrow(variables)) { variables &lt;- bind_rows( variables, as.data.frame(matrix(data =0, # fill with zeros nrow=max_steps, # max_steps rows ncol=5)) ) } step_idx &lt;- step_idx + 1 } # end for # drop lines that are empty as we exceeded # the time we needed to run this for return(as.data.frame(variables)) } # end SIR_Gillespie # specify the parameters parameters &lt;- c(beta = 2, sigma = 0.25, mu = 1e-05, lambda = 1e-05) initial_values &lt;- c(S = 5000, I = 1, R = 0) # run the Gillespie model first_result &lt;- SIR_Gillespie( params = parameters, initial_values = initial_values, max_time = 80) # our data is current in wide format # with each row consisting of time, T, S, I and R # but to plot things easily using ggplot2 # we want things in long format first_result_l &lt;- first_result %&gt;% pivot_longer(cols = c(S,I,R) ,names_to = &quot;Type&quot; ,values_to = &quot;Density&quot;) # plot the result ggplot(data=first_result_l, mapping = aes(x = time, y = Density)) + geom_line(mapping = aes(colour = Type)) + theme_classic() + scale_colour_brewer(palette=&quot;Set1&quot;) + xlab(&quot;Time&quot;) + xlim(0,80) 3.2.2 Task: running the model Run the model for a large population size. Start with \\(S=5000\\), which dependent on the speed of your computer may take a little while. Without changing the parameters, you may see sometimes see a plot that looks like this: You notice that there are no infected individuals! Why do you think this is so? Think about the role of chance effects covered in the lecture. 3.2.3 Task: running the above for small population sizes Now try the same but then for very small populations, i.e., \\(S = 50\\). What do you notice about the shapes of the lines? Are they still as smooth as for \\(S=5000\\). If you run this repeatedly, do you find that the population always gets infected? 3.2.4 Task: running and plotting replicate sims So far we just ran the same simulation a bunch of times by selecting the same code over and over again, which is slightly tedious. We can also build a for loop that allows us to obtain multiple results more quickly. We then combine these results in a big data frame. Note that each simulation run is uniquely numbered, so that we can keep track of which replicate is which when plotting the simulations. Take a look at the code below for 10 replicates: parameters &lt;- c(beta = 1,sigma=1/5,mu=1e-05,lambda=1e-05) init_dens &lt;- c(S = 200, I = 1, R = 0) tmax = 100 # allocate an empty dataframe # this will contain all the replicate simulations all_the_simulations &lt;- NULL # number of replicates number_replicates &lt;- 5 # use a for loop to gather replicate simulations for (replicate_sim_idx in 1:number_replicates) { # run a single replicate simulation result &lt;- SIR_Gillespie(params = parameters, initial_values = init_dens,max_time = tmax) # give this replicate simulation a column # called replicate_id, so that all rows # of this particular data set will have # the same replicate_id result &lt;- result %&gt;% mutate( replicate_id = as.character(replicate_sim_idx) # store as text so that it becomes a factor ) # append the result to the big dataset all_the_simulations &lt;- bind_rows( all_the_simulations,result ) } # end for loop ggplot(data = all_the_simulations ,mapping = aes(x = time, y = I)) + geom_line(mapping = aes( colour = replicate_id)) + theme_classic() One can also try to join up all the replicates of S, I and R in one plot, so that it looks like this: 3.2.5 Task: what happens when \\(R_{0} &lt; 1\\)? Using the code above, run the simulation for an initial susceptible density of \\(S = 100\\) and change the value of \\(\\sigma\\) and/or \\(\\beta\\) so that we have an R0 of 0.95 (i.e., the disease is not expected to invade according to all our deterministic models so far). Try to run 50 replicates. Do we indeed find that the disease is never able to invade? Discuss why your findings look like this. hint: consider the role of chance effects 3.2.6 Advanced task: systematically vary population size and look at extinction Warning: running this code can take quite a while We could also vary the population density (by means of varying the initial density of susceptibles \\(S\\)). We then can study the time it takes for a disease to go extinct in different populations. The code below does exactly this. Before going ahead and running it, what would you expect? Are diseases more likely to maintain themselves in large versus small populations? After running the code below, what do you conclude? Would a 10% increase in population density matter dramatically to disease extinction when the current population is already quite large? What about a 10% increase in size of a small population? parameters &lt;- c(beta = 1,sigma=1/15,mu=1e-05,lambda=1e-05) tmax = 150 # vary S as this sets the population size S &lt;- c(20,50,75,500,2000,4000) # number of replicates number_replicates &lt;- 5 # allocate data on extinction data_extinction &lt;- data.frame( run_id = 1:number_replicates * length(S) ) # allocate two additional column to put the extinction times in, the value of S data_extinction[,c(&quot;time_extinct&quot;,&quot;S&quot;)] &lt;- c(NA, NA) run_id &lt;- 1 # use a for loop to gather replicate simulations for (s_idx in 1:length(S)) { init_dens &lt;- c(S = S[s_idx], I = 1, R = 0) for (replicate_idx in 1:number_replicates) { # run a single replicate simulation result &lt;- SIR_Gillespie(params = parameters, initial_values = init_dens,max_time = tmax) # we want to find the time that I went extinct all_times_extinct &lt;- result %&gt;% filter(I == 0) %&gt;% select(time) if (nrow(all_times_extinct) == 0) { time_extinction = tmax } else { # find the earliest time that this happend time_extinction &lt;- min(all_times_extinct) } data_extinction[run_id ,c(&quot;S&quot;,&quot;time_extinct&quot;)] &lt;- c(S[s_idx],time_extinction) run_id &lt;- run_id + 1 } # end for replicate idx } #end for s_idx ggplot(data = data_extinction, ,mapping = aes(x = S, y = time_extinct)) + geom_point() + theme_classic() + xlab(&quot;Population size (init S)&quot;) + ylab(&quot;Time until disease extinct&quot;) "],["practical-fitting-dynamic-models-to-data.html", "Chapter 4 Practical: fitting dynamic models to data 4.1 Hong Kong flu data set 4.2 Task: calculate the sum of squares for your own wild guess of \\(\\beta\\) and \\(\\sigma\\) 4.3 Task: run optim() to find the best fit of \\(\\beta\\) and \\(\\sigma\\) 4.4 Plotting the data and the best-fit model 4.5 Task: missing data", " Chapter 4 Practical: fitting dynamic models to data 4.1 Hong Kong flu data set Please find a dataset on the 1968 Hong Kong flu outbreak in New York below: flu &lt;- data.frame(week = 1:13, deaths = c(14, 28, 50, 66, 156, 190, 156, 108, 68, 77, 33, 65, 24)) We will now use optim() to try and fit this dataset and estimate values for the transmission rate \\(\\beta\\) and the disease clearance rate \\(\\sigma\\). 4.1.1 Curve fitting code using optim() Copy-paste the code below and use it to get the values of \\(beta\\) and \\(\\sigma\\) that best the data. library(&quot;tidyverse&quot;) library(&quot;deSolve&quot;) # first add a variable that captures time in days # so that estimated rates are events / day instead of events /week flu &lt;- flu %&gt;% mutate( day = week * 7 ) # define the sir model as before, without demography sir_ode &lt;- function(t, state, parameters) { with(as.list(c(state, parameters)), { dS &lt;- - beta*I/(S+I+R)*S dI &lt;- beta*I/(S+I+R)*S - sigma*I dR &lt;- sigma*I return(list(c(dS,dI,dR))) }) } # our goal function calculates the # sum of squares of the difference # between model and data goal.function &lt;- function( parameters, initial_densities, data) { # first obtain the time points from the dataset #this produces a list of numbers 0, 0.1, 0.2, ..., 200 times_from_data = seq(0, max(data[,&quot;day&quot;]), by = 0.1) # solve the SIR ODE over time # store the result as a data.frame the.ode.data &lt;- as.data.frame(ode( y = initial_densities, times = times_from_data, func = sir_ode, parms = parameters)) # select I from the ode data set # only for those time steps # that are in the flu data as well I_ode &lt;- the.ode.data %&gt;% filter(time %in% (data[,&quot;day&quot;])) %&gt;% select(I) # now extract the numbers of infecteds from # the resulting ode data and compare to # the ODE # and calculate the sum of squares between # the data and the ODE SS &lt;- sum((I_ode - data[,&quot;deaths&quot;])^2) return(SS) } 4.2 Task: calculate the sum of squares for your own wild guess of \\(\\beta\\) and \\(\\sigma\\) Before we run optim() and find the value of \\(\\beta\\), try to run the goal function above, for your own value of \\(\\beta\\): pars &lt;- c(beta = 1, sigma = 0.8) init_dens &lt;- c(S=9999,I=1,R=0) goal.function(parameters = pars, initial_densities = init_dens, data = flu) What is the number that you obtain from your goal function? What does that number represent? 4.3 Task: run optim() to find the best fit of \\(\\beta\\) and \\(\\sigma\\) See the code below: # we need to provide optim() # with some initial guesses upon which # it tries to improve the fit pars &lt;- c(beta = 1, sigma = 0.8) init_dens &lt;- c(S=9999,I=1,R=0) # now use optim() to find the # values of beta and sigma # start with parameters above as an initial guess optim_result &lt;- optim(par = pars ,fn = goal.function ,initial_densities=init_dens ,data=flu) (optim_result) ## $par ## beta sigma ## 0.7938060 0.6405044 ## ## $value ## [1] 10430.7 ## ## $counts ## function gradient ## 75 NA ## ## $convergence ## [1] 0 ## ## $message ## NULL Look at what is contained in the variable optim_result: what are the values of beta and sigma that give the best fit? What is the resulting value of the sum of squares? 4.4 Plotting the data and the best-fit model # now we have the parameters that give the best # possible fit, solve the ode # given these parameters so that we can overlay # the solution with the data and visually # inspect the fit ode_optim_result &lt;- as.data.frame( ode(y = init_dens, func = sir_ode, parms = optim_result$par, times = seq(1, max(flu[,&quot;day&quot;]), length.out = 1000) ) ) ggplot(data = flu, mapping = aes(x = day, y = deaths)) + geom_point(size=1.5, color=&quot;darkred&quot;) + geom_line( data = ode_optim_result, mapping = aes(x = time, y = I) ) + theme_classic() 4.5 Task: missing data Let’s imagine that funds ran out to monitor this disease after week 7. As a consequence, the data set only contains rows up to and including week 7. The question is: is our estimate still as good as it was before? This is called cross-validation: how sensitive are your inferences/predictions robust when only half of the data is used? Hence the data set is now as follows: flu_first_half &lt;- flu %&gt;% filter(week &lt;= 5) optim_result &lt;- optim(par = pars ,fn = goal.function ,initial_densities=init_dens ,data=flu_first_half) What do you conclude about the robustness of your previous estimate? "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
